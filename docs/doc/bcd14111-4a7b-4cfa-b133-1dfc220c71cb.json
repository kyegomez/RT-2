{
    "summary": "The RT2 model is a neural network for image processing tasks utilizing transformers. It uses modules from the zeta library and has options for attention mechanisms and position embeddings, with encoder and decoder components.",
    "details": [
        {
            "comment": "The code defines a class RT2 which is a neural network model for image processing tasks. It takes in various parameters such as image_size, patch_size, etc., and initializes modules from the zeta library like Encoder, Decoder, Transformer, etc. The model consists of an encoder and a decoder part with specified dimensions, depths, and attention heads. It also has options for using absolute position embeddings and enabling cross-attention in the decoder.",
            "location": "\"/media/root/Prima/works/RT-2/docs/src/rt2/model.py\":0-28",
            "content": "import torch\nfrom torch import nn\nfrom zeta.structs import (\n    AutoregressiveWrapper,\n    Decoder,\n    Encoder,\n    Transformer,\n    ViTransformerWrapper,\n)\nclass RT2(nn.Module):\n    \"\"\"\n    RT2 model implementation.\n    Args:\n        image_size (int): Size of the input image.\n        patch_size (int): Size of each image patch.\n        encoder_dim (int): Dimension of the encoder.\n        encoder_depth (int): Depth of the encoder.\n        encoder_heads (int): Number of attention heads in the encoder.\n        num_tokens (int): Number of tokens in the decoder.\n        max_seq_len (int): Maximum sequence length in the decoder.\n        decoder_dim (int): Dimension of the decoder.\n        decoder_depth (int): Depth of the decoder.\n        decoder_heads (int): Number of attention heads in the decoder.\n        attn_kv_heads (int): Number of attention heads for key-value projection.\n        use_abs_pos_emb (bool): Whether to use absolute positional embeddings.\n        cross_attend (bool): Whether to enable cross-attention in the decoder."
        },
        {
            "comment": "This code defines a class called RT2 that inherits from some unspecified base class. It initializes various attributes, including an encoder module, a decoder module, and takes several parameters such as image_size, patch_size, etc. The encoder and decoder modules seem to be instances of the classes Encoder and AutoregressiveWrapper, respectively. The code also includes various boolean flags like attn_flash and qk_norm that control certain attention mechanisms in the model. Overall, this seems to be a definition of a transformer-based model for image processing tasks.",
            "location": "\"/media/root/Prima/works/RT-2/docs/src/rt2/model.py\":29-62",
            "content": "        attn_flash (bool): Whether to enable flash attention in the decoder.\n        qk_norm (bool): Whether to normalize queries and keys in attention.\n    Attributes:\n        encoder (ViTransformerWrapper): Encoder module.\n        decoder (AutoregressiveWrapper): Decoder module.\n    \"\"\"\n    def __init__(\n        self,\n        image_size: int = 256,\n        patch_size: int = 32,\n        encoder_dim: int = 512,\n        encoder_depth: int = 6,\n        encoder_heads: int = 8,\n        num_tokens: int = 20000,\n        max_seq_len: int = 1024,\n        decoder_dim: int = 512,\n        decoder_depth: int = 6,\n        decoder_heads: int = 8,\n        attn_kv_heads: int = 2,\n        use_abs_pos_emb: bool = False,\n        cross_attend: bool = True,\n        attn_flash: bool = True,\n        qk_norm: bool = True,\n    ):\n        super(RT2, self).__init__()\n        self.encoder = ViTransformerWrapper(\n            image_size=image_size,\n            patch_size=patch_size,\n            attn_layers=Encoder(\n                dim=encoder_dim, depth=encoder_depth, heads=encoder_heads"
        },
        {
            "comment": "The code defines a class for the RT2 model, which consists of an encoder and decoder. The encoder takes in an image tensor and returns encoded embeddings. The decoder is a transformer with customizable layers and parameters. The forward function performs the forward pass for the model by calling the encoder and returning the output.",
            "location": "\"/media/root/Prima/works/RT-2/docs/src/rt2/model.py\":63-99",
            "content": "            ),\n        )\n        self.decoder = Transformer(\n            num_tokens=num_tokens,\n            max_seq_len=max_seq_len,\n            use_abs_pos_emb=use_abs_pos_emb,\n            attn_layers=Decoder(\n                dim=decoder_dim,\n                depth=decoder_depth,\n                heads=decoder_heads,\n                cross_attend=cross_attend,\n                attn_kv_heads=attn_kv_heads,\n                attn_flash=attn_flash,\n                qk_norm=qk_norm,\n            ),\n        )\n        self.decoder = AutoregressiveWrapper(self.decoder)\n    def forward(self, img: torch.Tensor, text: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass of the RT2 model.\n        Args:\n            img (torch.Tensor): Input image tensor.\n            text (torch.Tensor): Input text tensor.\n        Returns:\n            torch.Tensor: Output tensor.\n        Raises:\n            Exception: If an error occurs during the forward pass.\n        \"\"\"\n        try:\n            encoded = self.encoder(img, return_embeddings=True)"
        },
        {
            "comment": "The code snippet is the implementation of a method, likely in a class-based model. It attempts to return the result of calling the 'decoder' function on 'text' and 'encoded'. If an exception occurs during this process, it prints an error message with details about the failure and then re-raises the exception.",
            "location": "\"/media/root/Prima/works/RT-2/docs/src/rt2/model.py\":100-103",
            "content": "            return self.decoder(text, context=encoded)\n        except Exception as error:\n            print(f\"Failed in forward method: {error}\")\n            raise"
        }
    ]
}